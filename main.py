# -*- coding: utf-8 -*-
"""Untitled31.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Re1EUIuAKiT-WcwxT7sviArVyRDiVMjD
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
import matplotlib.pyplot as plt

# Load Dataset
df = pd.read_csv("IMDB Dataset.csv")

# Encode labels: positive=1, negative=0
le = LabelEncoder()
df['sentiment'] = le.fit_transform(df['sentiment'])

# Parameters
max_words = 10000
max_len = 200

# Tokenize text
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(df['review'])
X = tokenizer.texts_to_sequences(df['review'])
X = pad_sequences(X, maxlen=max_len)
y = df['sentiment'].values

# Train/test split
X_train, X_test, y_train, y_test, text_train, text_test = train_test_split(
    X, y, df['review'], test_size=0.1, random_state=42)

# Build RNN Model
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))
model.add(SimpleRNN(64))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train
history = model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.1)


# Save the model
model.save("sentiment_rnn_model.h5")

# Save the tokenizer
from tensorflow.keras.preprocessing.text import tokenizer_from_json
import json

tokenizer_json = tokenizer.to_json()
with open("tokenizer.json", "w") as f:
    f.write(tokenizer_json)

# Predict
pred_probs = model.predict(X_test)
preds = (pred_probs > 0.5).astype(int).flatten()

# Decode function
decode_sentiment = lambda label: 'positive' if label == 1 else 'negative'

# Show all input + output
print("\n=== Review Predictions ===")
for i in range(len(preds)):
    print(f"\nReview #{i+1}")
    print("Text     :", text_test.iloc[i][:300] + "...")
    print("Actual   :", decode_sentiment(y_test[i]))
    print("Predicted:", decode_sentiment(preds[i]))

# Accuracy
accuracy = np.mean(preds == y_test)
print(f"\n Test Accuracy: {accuracy * 100:.2f}%")




#visualisation

# Plot Accuracy and Loss
plt.figure(figsize=(14, 5))

# Plot Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

